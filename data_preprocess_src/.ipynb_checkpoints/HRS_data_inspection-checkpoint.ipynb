{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.6 (v3.8.6:db455296be, Sep 23 2020, 13:31:39) \\n[Clang 6.0 (clang-600.0.57)]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path.cwd().parent)\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src import DataImport\n",
    "import matplotlib.pyplot as plt\n",
    "from src import Models\n",
    "import shap\n",
    "import pandas as pd \n",
    "import xgboost\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from src import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ --------\n",
      "anyio                    3.6.2\n",
      "appnope                  0.1.3\n",
      "argon2-cffi              21.3.0\n",
      "argon2-cffi-bindings     21.2.0\n",
      "arrow                    1.2.3\n",
      "astor                    0.8.1\n",
      "asttokens                2.2.1\n",
      "attrs                    22.2.0\n",
      "autograd                 1.5\n",
      "autograd-gamma           0.5.0\n",
      "backcall                 0.2.0\n",
      "beautifulsoup4           4.11.2\n",
      "bleach                   6.0.0\n",
      "cffi                     1.15.1\n",
      "cloudpickle              2.2.1\n",
      "comm                     0.1.2\n",
      "contourpy                1.0.7\n",
      "cycler                   0.11.0\n",
      "debugpy                  1.6.6\n",
      "decorator                5.1.1\n",
      "defusedxml               0.7.1\n",
      "executing                1.2.0\n",
      "fastjsonschema           2.16.2\n",
      "fonttools                4.38.0\n",
      "formulaic                0.5.2\n",
      "fqdn                     1.5.1\n",
      "future                   0.18.3\n",
      "graphlib-backport        1.0.3\n",
      "idna                     3.4\n",
      "importlib-metadata       6.0.0\n",
      "importlib-resources      5.12.0\n",
      "interface-meta           1.3.0\n",
      "ipykernel                6.21.2\n",
      "ipython                  8.10.0\n",
      "ipython-genutils         0.2.0\n",
      "ipywidgets               8.0.4\n",
      "isoduration              20.11.0\n",
      "jedi                     0.18.2\n",
      "Jinja2                   3.1.2\n",
      "joblib                   1.2.0\n",
      "jsonpointer              2.3\n",
      "jsonschema               4.17.3\n",
      "jupyter                  1.0.0\n",
      "jupyter_client           8.0.3\n",
      "jupyter-console          6.6.1\n",
      "jupyter_core             5.2.0\n",
      "jupyter-events           0.6.3\n",
      "jupyter_server           2.3.0\n",
      "jupyter_server_terminals 0.4.4\n",
      "jupyterlab-pygments      0.2.2\n",
      "jupyterlab-widgets       3.0.5\n",
      "kiwisolver               1.4.4\n",
      "lifelines                0.27.4\n",
      "lightgbm                 3.3.5\n",
      "llvmlite                 0.39.1\n",
      "MarkupSafe               2.1.2\n",
      "matplotlib               3.7.0\n",
      "matplotlib-inline        0.1.6\n",
      "missingpy                0.2.0\n",
      "mistune                  2.0.5\n",
      "nbclassic                0.5.2\n",
      "nbclient                 0.7.2\n",
      "nbconvert                7.2.9\n",
      "nbformat                 5.7.3\n",
      "nest-asyncio             1.5.6\n",
      "notebook                 6.5.2\n",
      "notebook_shim            0.2.2\n",
      "numba                    0.56.4\n",
      "numpy                    1.23.5\n",
      "packaging                23.0\n",
      "pandas                   1.5.3\n",
      "pandocfilters            1.5.0\n",
      "parso                    0.8.3\n",
      "pexpect                  4.8.0\n",
      "pickleshare              0.7.5\n",
      "Pillow                   9.4.0\n",
      "pip                      21.3.1\n",
      "pkgutil_resolve_name     1.3.10\n",
      "platformdirs             3.0.0\n",
      "prometheus-client        0.16.0\n",
      "prompt-toolkit           3.0.37\n",
      "psutil                   5.9.4\n",
      "ptyprocess               0.7.0\n",
      "pure-eval                0.2.2\n",
      "pycparser                2.21\n",
      "Pygments                 2.14.0\n",
      "pyparsing                3.0.9\n",
      "pyrsistent               0.19.3\n",
      "python-dateutil          2.8.2\n",
      "python-json-logger       2.0.7\n",
      "pytz                     2022.7.1\n",
      "PyYAML                   6.0\n",
      "pyzmq                    25.0.0\n",
      "qtconsole                5.4.0\n",
      "QtPy                     2.3.0\n",
      "rfc3339-validator        0.1.4\n",
      "rfc3986-validator        0.1.1\n",
      "scikit-learn             1.2.1\n",
      "scipy                    1.10.1\n",
      "Send2Trash               1.8.0\n",
      "setuptools               60.2.0\n",
      "shap                     0.41.0\n",
      "six                      1.16.0\n",
      "slicer                   0.0.7\n",
      "sniffio                  1.3.0\n",
      "soupsieve                2.4\n",
      "stack-data               0.6.2\n",
      "terminado                0.17.1\n",
      "threadpoolctl            3.1.0\n",
      "tinycss2                 1.2.1\n",
      "tornado                  6.2\n",
      "tqdm                     4.64.1\n",
      "traitlets                5.9.0\n",
      "typing_extensions        4.5.0\n",
      "uri-template             1.2.0\n",
      "wcwidth                  0.2.6\n",
      "webcolors                1.12\n",
      "webencodings             0.5.1\n",
      "websocket-client         1.5.1\n",
      "wheel                    0.37.1\n",
      "widgetsnbextension       4.0.5\n",
      "wrapt                    1.14.1\n",
      "xgboost                  1.7.4\n",
      "zipp                     3.14.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/valler/Python/OX_Thesis/OX_thesis/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dfMort imputed on 22th Feb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "from missingpy import MissForest\n",
    "\n",
    "df=pd.read_csv(Path.cwd().parent/'Data/HRS/data_preprocess/Data/merge_data_step_3.csv')\n",
    "df['death'] = [0 if np.isnan(x) else 1 for x in df['deathYear']]\n",
    "df=df.dropna(thresh=50)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df_to_missing_forest=df.drop(columns=['death','deathYear','deathMonth','deathReason','year_check','hhidpn','hhid','pn'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9803\n",
       "1    4087\n",
       "Name: death, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['death'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing forest\n",
    "df_to_missing_forest=df.drop(columns=['death','deathYear','deathMonth','deathReason','year_check','hhidpn','hhid','pn'])\n",
    "imputer = MissForest(n_estimators=500, max_iter=5,criterion='squared_error')\n",
    "imputed_data = imputer.fit_transform(df_to_missing_forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = pd.DataFrame(columns=df_to_missing_forest.columns,data=imputed_data)\n",
    "df_imputed.to_csv(Path.cwd().parent/'Data/HRS/data_preprocess/missingforest/df_by_us_no_missing_computed_20230226_before_merge.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4x/vv626j3d62g57l8x8_7ksf0r0000gn/T/ipykernel_1738/210735609.py:3: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data_non_missing_version_2.loc[:,['death','deathYear','deathMonth','deathReason','year_check','hhidpn','hhid','pn']]=df.loc[:,['death','deathYear','deathMonth','deathReason','year_check','hhidpn','hhid','pn']]\n"
     ]
    }
   ],
   "source": [
    "###### data_non_missing_version_2 = pd.DataFrame(columns=df_to_missing_forest.columns,data=imputed_data)\n",
    "data_non_missing_version_2=df_imputed.copy()\n",
    "data_non_missing_version_2.loc[:,['death','deathYear','deathMonth','deathReason','year_check','hhidpn','hhid','pn']]=df.loc[:,['death','deathYear','deathMonth','deathReason','year_check','hhidpn','hhid','pn']]\n",
    "data_non_missing_version_2.to_csv(Path.cwd().parent/'Data/HRS/data_preprocess/missingforest/df_by_us_no_missing_computed_20230226_after_merge.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9803\n",
       "1    4087\n",
       "Name: death, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_non_missing_version_2['death'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binary vars treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "before: \n",
      "-1.0    8291\n",
      " 1.0    5599\n",
      "Name: maleYN, dtype: int64\n",
      "[[-1.0, 8291], [1.0, 5599]]\n",
      "cut_point {-1.0: 0.19380849532037447}\n",
      "bins [-99.80619150467963, 0.19380849532037447, 100.19380849532037]\n",
      "after: -1.0    8291\n",
      " 1.0    5599\n",
      "Name: maleYN, dtype: int64\n",
      "\n",
      "before: \n",
      "-1.000    12138\n",
      " 1.000     1751\n",
      "-0.152        1\n",
      "Name: blackYN, dtype: int64\n",
      "[[-1.0, 12138], [1.0, 1751]]\n",
      "cut_point {-1.0: 0.7478580171358629}\n",
      "bins [-99.25214198286413, 0.7478580171358629, 100.74785801713587]\n",
      "after: -1.0    12139\n",
      " 1.0     1751\n",
      "Name: blackYN, dtype: int64\n",
      "\n",
      "before: \n",
      " 1.000    12592\n",
      "-1.000     1284\n",
      " 0.720        2\n",
      " 0.544        1\n",
      " 0.128        1\n",
      " 0.856        1\n",
      " 0.740        1\n",
      " 0.948        1\n",
      " 0.644        1\n",
      " 0.872        1\n",
      " 0.568        1\n",
      " 0.972        1\n",
      "-0.148        1\n",
      " 0.876        1\n",
      "-0.488        1\n",
      "Name: migrantYN, dtype: int64\n",
      "[[1.0, 12592], [-1.0, 1284]]\n",
      "cut_point {-1.0: -0.8149322571346209}\n",
      "bins [-100.81493225713461, -0.8149322571346209, 99.18506774286539]\n",
      "after:  1.0    12606\n",
      "-1.0     1284\n",
      "Name: migrantYN, dtype: int64\n",
      "\n",
      "before: \n",
      " 1.0    10712\n",
      "-1.0     3178\n",
      "Name: modactivityYN, dtype: int64\n",
      "[[1.0, 10712], [-1.0, 3178]]\n",
      "cut_point {-1.0: -0.5424046076313895}\n",
      "bins [-100.54240460763138, -0.5424046076313895, 99.45759539236862]\n",
      "after:  1.0    10712\n",
      "-1.0     3178\n",
      "Name: modactivityYN, dtype: int64\n",
      "\n",
      "before: \n",
      "-1.000    9703\n",
      " 1.000    3879\n",
      "-0.412       4\n",
      "-0.684       4\n",
      "-0.136       4\n",
      "          ... \n",
      "-0.496       1\n",
      " 0.096       1\n",
      "-0.144       1\n",
      "-0.880       1\n",
      "-0.192       1\n",
      "Name: sleepYN, Length: 183, dtype: int64\n",
      "[[-1.0, 9703], [1.0, 3879]]\n",
      "cut_point {-1.0: 0.42880282727138863}\n",
      "bins [-99.57119717272862, 0.42880282727138863, 100.42880282727138]\n",
      "after: -1.0    10009\n",
      " 1.0     3881\n",
      "Name: sleepYN, dtype: int64\n",
      "\n",
      "before: \n",
      " 1.000    7804\n",
      "-1.000    5983\n",
      " 0.676       4\n",
      " 0.600       3\n",
      " 0.672       3\n",
      "          ... \n",
      "-0.144       1\n",
      " 0.276       1\n",
      "-0.040       1\n",
      "-0.512       1\n",
      "-0.452       1\n",
      "Name: eversmokeYN, Length: 88, dtype: int64\n",
      "[[1.0, 7804], [-1.0, 5983]]\n",
      "cut_point {-1.0: -0.1320809458185247}\n",
      "bins [-100.13208094581853, -0.1320809458185247, 99.86791905418147]\n",
      "after:  1.0    7890\n",
      "-1.0    6000\n",
      "Name: eversmokeYN, dtype: int64\n",
      "\n",
      "before: \n",
      "-1.000    12137\n",
      " 1.000     1669\n",
      "-0.580        3\n",
      "-0.364        2\n",
      "-0.624        2\n",
      "          ...  \n",
      "-0.356        1\n",
      "-0.600        1\n",
      "-0.960        1\n",
      "-0.248        1\n",
      "-0.980        1\n",
      "Name: currsmokeYN, Length: 71, dtype: int64\n",
      "[[-1.0, 12137], [1.0, 1669]]\n",
      "cut_point {-1.0: 0.7582210633058091}\n",
      "bins [-99.24177893669419, 0.7582210633058091, 100.75822106330581]\n",
      "after: -1.0    12221\n",
      " 1.0     1669\n",
      "Name: currsmokeYN, dtype: int64\n",
      "\n",
      "before: \n",
      "-1.0    12741\n",
      " 1.0     1149\n",
      "Name: hispanicYN, dtype: int64\n",
      "[[-1.0, 12741], [1.0, 1149]]\n",
      "cut_point {-1.0: 0.8345572354211663}\n",
      "bins [-99.16544276457883, 0.8345572354211663, 100.83455723542117]\n",
      "after: -1.0    12741\n",
      " 1.0     1149\n",
      "Name: hispanicYN, dtype: int64\n",
      "\n",
      "before: \n",
      " 1.0    10712\n",
      "-1.0     3178\n",
      "Name: vigactivityYN, dtype: int64\n",
      "[[1.0, 10712], [-1.0, 3178]]\n",
      "cut_point {-1.0: -0.5424046076313895}\n",
      "bins [-100.54240460763138, -0.5424046076313895, 99.45759539236862]\n",
      "after:  1.0    10712\n",
      "-1.0     3178\n",
      "Name: vigactivityYN, dtype: int64\n",
      "\n",
      "before: \n",
      "-1.000    12901\n",
      " 1.000      985\n",
      "-0.592        1\n",
      "-0.696        1\n",
      "-0.648        1\n",
      "-0.448        1\n",
      "Name: alcoholYN, dtype: int64\n",
      "[[-1.0, 12901], [1.0, 985]]\n",
      "cut_point {-1.0: 0.8581304911421577}\n",
      "bins [-99.14186950885784, 0.8581304911421577, 100.85813049114216]\n",
      "after: -1.0    12905\n",
      " 1.0      985\n",
      "Name: alcoholYN, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(Path.cwd().parent/'Data/HRS/data_preprocess/missingforest/df_by_us_no_missing_computed_20230226_after_merge.csv')\n",
    "\n",
    "binary_columns = [x for x in df.columns if 'YN' in x]\n",
    "for column in binary_columns:\n",
    "    print(f'\\nbefore: \\n{df[column].value_counts()}')\n",
    "    temp=recode_categorical_vars(column,2,df)\n",
    "    print(f'after: {df[column].value_counts()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_99_qr=np.percentile(df['ZincomeT'], [99.99 ,25])[0]\n",
    "df['ZincomeT'] = np.where(df['ZincomeT'] >= income_99_qr, income_99_qr, df['ZincomeT'])\n",
    "wealth_99_qr=np.percentile(df['ZwealthT'], [99.99 ,25])[0]\n",
    "df['ZwealthT'] = np.where(df['ZwealthT'] >= wealth_99_qr, wealth_99_qr, df['ZwealthT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(Path.cwd().parent/'Data/HRS/model_used_data/df_by_us.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import Models\n",
    "from src import DataImport\n",
    "from src import Evaluate\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "domains = DataImport.domain_dict()\n",
    "\n",
    "df=pd.read_csv(Path.cwd()/'Data/HRS/model_used_data/df_by_us.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xgb\n",
      "imv=0.11578468662965002,\n",
      "roc-auc=0.7972320353537359,\n",
      "pr-auc=0.6339765459765514,\n",
      "f1=0.5588768115942029,\n",
      "efron_r2=0.2029581429135887,\n",
      "ffc_r2=0.5348794969908781,\n",
      "IP=0.292913709760362\n",
      "\n",
      "lgb\n",
      "imv=0.16898376486769684,\n",
      "roc-auc=0.8143639436880615,\n",
      "pr-auc=0.6546368192665374,\n",
      "f1=0.5446848541862652,\n",
      "efron_r2=0.2608691927482608,\n",
      "ffc_r2=0.5686739789110025,\n",
      "IP=0.292913709760362\n"
     ]
    }
   ],
   "source": [
    "for model_selection in ['xgb','lgb']:\n",
    "    print(f'\\n{model_selection}')\n",
    "    model = Models.Model_fixed_test_size(data=df, test_size=0.3,domain_list=domains['all'], model=model_selection,train_subset_size=1, order=0, y_colname='death')\n",
    "    evas = Evaluate.metric(model)\n",
    "    print(f'imv={evas.imv},\\nroc-auc={evas.auc_score},\\npr-auc={evas.pr_auc},\\nf1={evas.pr_f1},\\nefron_r2={evas.efron_rsquare},\\nffc_r2={evas.ffc_r2},\\nIP={evas.pr_no_skill}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bio Domain Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src import DataImport\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "from missingpy import MissForest\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Step1: read data\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "bio_08 = pd.read_stata('/Users/valler/OneDrive - Nexus365/Dissertation/biomarker/BIOMK08BL_R.dta')\n",
    "bio_06 = pd.read_stata('/Users/valler/OneDrive - Nexus365/Dissertation/biomarker/BIOMK06BL_R.dta')\n",
    "\n",
    "df = DataImport.data_reader_by_us(bio=False)\n",
    "\n",
    "# Step1 End Here ---------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Step2: combine HHID and PN together for df, bio_06 and bio_08\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "df['hhidpn']=df['hhidpn'].astype(int)\n",
    "df['hhid']=df['hhid'].astype(int)\n",
    "\n",
    "bio_06['HHID'] = bio_06['HHID'].astype(int)\n",
    "bio_06['PN'] = bio_06['PN'].astype(int)\n",
    "\n",
    "bio_08['HHID'] = bio_08['HHID'].astype(int)\n",
    "bio_08['PN'] = bio_08['PN'].astype(int)\n",
    "\n",
    "bio_06['hhidpn']=[str(bio_06.loc[index,'HHID'])+'0'+str(bio_06.loc[index,'PN']) for index in bio_06.index]\n",
    "bio_08['hhidpn']=[str(bio_08.loc[index,'HHID'])+'0'+str(bio_08.loc[index,'PN']) for index in bio_08.index]\n",
    "\n",
    "bio_06['hhidpn'] = bio_06['hhidpn'].astype(int)\n",
    "bio_08['hhidpn'] = bio_08['hhidpn'].astype(int)\n",
    "# Step2 End Here ---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "# Step3: preprocess bio_06 and bio_08\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# For bio_06, only keep the following columns\n",
    "bio_06_columns = ['hhidpn', 'KA1CBIOS', 'KA1C_ADJ',\n",
    "                  'KHDlBIOS', 'KHDl_ADJ',\n",
    "                  'KTCBIOS', 'KTC_ADJ',\n",
    "                  'KCYSC_IMP', 'KCYSC_ADJ',\n",
    "                  'KCRP_IMP', 'KCRP_ADJ',\n",
    "                  'KBlVERSION', 'KBIOWGTR']\n",
    "\n",
    "bio_06=bio_06[bio_06_columns]\n",
    "\n",
    "bio_06_columns = [x.replace('K','').replace('BIOS','').replace('l','L') for x in bio_06.columns]\n",
    "bio_06_col_dict = {x:y for x,y in zip(list(bio_06.columns),bio_06_columns)}\n",
    "bio_06=bio_06.rename(columns=bio_06_col_dict)\n",
    "\n",
    "\n",
    "\n",
    "# construct 3 key variables for bio_08 (non-adjusted, multiple data sources)\n",
    "# 1. TC, missing = 299\n",
    "bio_08['TC'] = [x if np.isnan(y) else y for x,y in zip(bio_08['LTCUW'],bio_08['LTCBIOS'])]\n",
    "bio_08['TC'].isna().sum()\n",
    "\n",
    "# 2. HDL , missing = 776\n",
    "bio_08['HDL'] = [x if np.isnan(y) else y for x,y in zip(bio_08['LHDLBIOS'],bio_08['LHDLUW'])]\n",
    "bio_08['HDL'].isna().sum()\n",
    "\n",
    "# 3. HbA1c , missing = 73\n",
    "bio_08['A1C'] = [x if np.isnan(y) else y for x,y in zip(bio_08['LA1CBIOS'],bio_08['LA1CFLEX'])]\n",
    "bio_08['A1C'].isna().sum()\n",
    "\n",
    "bio_08_columns = ['hhidpn','A1C', 'LA1C_ADJ',\n",
    "                  'HDL', 'LHDL_ADJ',\n",
    "                  'TC','LTC_ADJ',\n",
    "                  'LCYSC_IMP','LCYSC_ADJ',\n",
    "                  'LCRP_IMP','LCRP_ADJ',\n",
    "                  'LBLVERSION', 'LBIOWGTR']\n",
    "\n",
    "bio_08 = bio_08[bio_08_columns]\n",
    "bio_08_columns = [x[1:] if x.startswith('L') else x for x in bio_08.columns]\n",
    "bio_08_col_dict = {x:y for x,y in zip(list(bio_08.columns),bio_08_columns)}\n",
    "bio_08=bio_08.rename(columns=bio_08_col_dict)\n",
    "#bio_08.columns\n",
    "\n",
    "bio = pd.concat([bio_08,bio_06],axis=0)\n",
    "# bio.to_csv(Path.cwd()/'model_used_data/bio_all.csv')\n",
    "#columns: 'A1C', 'A1C_ADJ', 'HDL', 'HDL_ADJ', 'TC', 'TC_ADJ',\n",
    "#'CYSC_IMP', 'CYSC_ADJ', 'CRP_IMP', 'CRP_ADJ', 'BLVERSION', 'BIOWGTR'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "# Step4: merge bio_06,bio_08 and df to bio_all_raw and check duplicates of hhidpn\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "# combine 06 and 08 with df separately\n",
    "combined_06 = df.merge(bio_06, left_on=['hhidpn'], right_on=['hhidpn'])\n",
    "combined_08 = df.merge(bio_08, left_on=['hhidpn'], right_on=['hhidpn'])\n",
    "column_difference = list(set(combined_08.columns)-set(combined_06.columns))\n",
    "bio_all_raw = pd.concat([combined_08, combined_06], axis=0)\n",
    "\n",
    "\n",
    "# bio_all_raw.to_csv(Path.cwd()/'model_used_data/bio_all_with_df_raw.csv')\n",
    "\n",
    "print('bio_all_raw has {} rows and df has {} rows'.format(len(bio_all_raw),len(df)))\n",
    "\n",
    "unique_hhidpn = list(set(bio_all_raw['hhidpn']))\n",
    "print(\"no overlapped hhidpn in the bio_all_raw, since unique_hhidpn = bio_all_raw = {}\".format(len(unique_hhidpn)))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# TODO: reconstruct the sample weight?\n",
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "# Step5: missing value imputation\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# check missing\n",
    "missing_columns = []\n",
    "for column in bio_all_raw.columns:\n",
    "    missings = bio_all_raw[column].isna().sum()\n",
    "    if (missings > 10) & ('death' not in column):\n",
    "        missing_columns.append(column)\n",
    "        print('for column {}, there are {} missing'.format(column, missings))\n",
    "\n",
    "# Make an instance and perform the imputation\n",
    "# first drop death related variables ['death','death_year','deathYR','death_month'],78 columns left\n",
    "bio_all_raw_columns = [x if 'death' not in x else 0 for x in bio_all_raw.columns]\n",
    "while 0 in bio_all_raw_columns:\n",
    "    bio_all_raw_columns.remove(0)\n",
    "\n",
    "bio_all_raw_columns_no_deaths = bio_all_raw[bio_all_raw_columns]\n",
    "# bio_all_raw_columns_no_deaths.to_csv(Path.cwd()/'model_used_data/bio_all_raw_columns_no_deaths.csv')\n",
    "random.seed(2022)\n",
    "\n",
    "bio_all_raw_columns_no_deaths=pd.read_csv(Path.cwd()/'model_used_data/bio_all_raw_columns_no_deaths.csv',index_col=0)\n",
    "imputer = MissForest(n_estimators=500, max_iter=5)\n",
    "bio_all_raw_columns_no_deaths_no_missing = imputer.fit_transform(bio_all_raw_columns_no_deaths)\n",
    "# missing values are computed at BMRC\n",
    "\n",
    "\n",
    "# computation method 1: compute\n",
    "no_missing = pd.DataFrame(data=bio_all_raw_columns_no_deaths_no_missing, columns=bio_all_raw_columns_no_deaths.columns)\n",
    "# no_missing['index']=no_missing['index'].astype(int)\n",
    "\n",
    "# computation method 2\n",
    "\n",
    "no_missing[['death','death_year','deathYR','death_month','deathReason']]=df[['death','death_year','deathYR','death_month','deathReason']]\n",
    "no_missing.to_csv(Path.cwd()/'model_used_data/df_by_us_bio.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Comparison - Figs used in the thesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from src import Models\n",
    "from src import Evaluate\n",
    "from src import DataImport\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = DataImport.data_reader(bio=False)\n",
    "df_bio = DataImport.data_reader(bio=True)\n",
    "domains = DataImport.domain_dict()\n",
    "df_by_us = DataImport.data_reader_by_us(bio=False)\n",
    "df_by_us_bio = DataImport.data_reader_by_us(bio=True)\n",
    "\n",
    "model_name = 'xgb'\n",
    "# non-bio\n",
    "# note that if you want to run the lgb model, you have to change the kernel to nlp\n",
    "df.drop(columns=['sampWeight'], inplace=True)\n",
    "\n",
    "model = Models.Model_fixed_test_size(data=df, test_size=0.3, domain_list=domains['all'], model=model_name,train_subset_size=1,order=0, y_colname='death')\n",
    "\n",
    "evaluate = Evaluate.metric(model=model)\n",
    "\n",
    "model_2 = Models.Model_fixed_test_size(data=df_by_us, test_size=0.3, domain_list=domains['all'], model=model_name,train_subset_size=1,order=0, y_colname='death')\n",
    "evaluate_2 = Evaluate.metric(model=model_2)\n",
    "\n",
    "\n",
    "# bio\n",
    "model_2 = Models.Model_fixed_test_size(data=df_by_us_bio, test_size=0.3, domain_list=domains['all_bio'], model=model_name,train_subset_size=1,order=0, y_colname='death')\n",
    "evaluate_2 = Evaluate.metric(model=model_2)\n",
    "\n",
    "model_3 = Models.Model_fixed_test_size(data=df_by_us_bio, test_size=0.3, domain_list=domains['all'], model=model_name,train_subset_size=1,order=0, y_colname='death')\n",
    "evaluate_3 = Evaluate.metric(model=model_3)\n",
    "\n",
    "# exam the columns that we are interested\n",
    "def compare_column(column,df,df_by_us):\n",
    "\n",
    "    # df_by_us=dfMort.copy()\n",
    "    # df_by_us['death']=[1 if (x==1)& (y<=2015) else 0 for x,y in zip(df_by_us['death'],df_by_us['deathYR'])]\n",
    "    compare=pd.DataFrame(columns=['hhidpn','interview_year',column+'_author',column+'_us','deathYR_author','deathYR_us'])\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        hhidpn=row['hhidpn']\n",
    "        if hhidpn in list(df_by_us.hhidpn):\n",
    "            row_by_us=df_by_us.loc[df_by_us['hhidpn']==hhidpn,]\n",
    "            value_by_us=row_by_us[column].values[0]\n",
    "            interview_year = row_by_us['interview_year'].values[0]\n",
    "\n",
    "            row_dict={'hhidpn':hhidpn,'interview_year': interview_year,column+'_author':row[column],column+'_us':value_by_us,\n",
    "                      'deathYR_author':row['deathYR'], 'deathYR_us':row_by_us['deathYR'].values[0]}\n",
    "            compare=compare.append(row_dict,ignore_index=True)\n",
    "\n",
    "\n",
    "    compare['equal']=[1 if x==y else 0 for x,y in zip(compare[column+'_author'],compare[column+'_us'])]\n",
    "\n",
    "\n",
    "    print(compare['equal'].sum(),12982-compare['equal'].sum())\n",
    "    return compare\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def characteristic_compare(df_by_us, df_author):\n",
    "    null_records=pd.DataFrame(columns=['column','dataset_mark','max','min','mean','uniques','null'])\n",
    "\n",
    "    count=0\n",
    "    df_by_us.rename(columns={'death':'deathYN','deathYear':'death_year','deathMonth':'death_month','ZwealthT':'Zwealth', 'ZincomeT':'Zincome'},inplace=True)\n",
    "    for column in df_by_us.columns:\n",
    "        if column in list(df_author.columns):\n",
    "\n",
    "            author_column  = df_author.loc[:,column].replace({' ':None,'True':1,'False':0}).astype(float)\n",
    "            null_records = null_records.append({'column':column, 'dataset_mark':'author', 'max':max(author_column), 'min':min(author_column), 'mean':np.mean(author_column),'uniques':len(author_column.unique()), 'null':author_column.isnull().sum()},\n",
    "                                               ignore_index=True)\n",
    "\n",
    "            count+=1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        our_column=df_by_us[column]\n",
    "        our_column = our_column.replace({' ': None, 'True': 1, 'False': 0}).astype(float)\n",
    "        null_records=null_records.append({'column':column,'dataset_mark':'us','max':max(our_column),'min':min(our_column), 'mean':np.mean(our_column),'uniques':len(our_column.unique()),'null':our_column.isnull().sum()},ignore_index=True)\n",
    "\n",
    "    return null_records\n",
    "\n",
    "null_records = characteristic_compare(df_by_us,df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures-fig1_death_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_yellow='#F1A52C'\n",
    "color_blue='#001C5B'\n",
    "# ------------------------\n",
    "# Figure1\n",
    "# ------------------------\n",
    "fontsize_ticks = 12\n",
    "fontsize_labels = 13\n",
    "\n",
    "df_deaths= pd.DataFrame(df_by_us[['death','age']].value_counts())\n",
    "df_deaths=df_deaths.reset_index(drop=False)\n",
    "df_deaths=df_deaths.loc[df_deaths['death']==1,]\n",
    "\n",
    "\n",
    "df_by_us['deathYR_half']=[x if y<=6 else x+0.5 for x,y in zip(df_by_us['death_year'],df_by_us['death_month'])]\n",
    "df_deaths_time=pd.DataFrame(df_by_us[['death','deathYR_half','interview_year']].value_counts())\n",
    "df_deaths_time=df_deaths_time.reset_index(drop=False)\n",
    "df_deaths_time=df_deaths_time.loc[df_deaths_time['death']==1,]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(left=0.08,right=0.95,top=0.95,bottom=0.1)\n",
    "# row=0\n",
    "ax1 = plt.subplot2grid((3, 4), (0, 0), colspan=4)\n",
    "ax1.bar(df_deaths['age'],df_deaths[0],color=color_blue,alpha=0.8)\n",
    "ax1.set_xlabel('Age')\n",
    "ax1.set_title('(a) Death Occurence in age')\n",
    "ax1.set_ylabel('Death Occurrence',size=fontsize_labels)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=fontsize_ticks)\n",
    "\n",
    "# row=1,0\n",
    "ax2 = plt.subplot2grid((3, 4), (1, 0), colspan=2)\n",
    "temp=df_deaths_time.loc[df_deaths_time['interview_year']==2006,]\n",
    "temp=temp.sort_values(by='deathYR_half')\n",
    "temp['cumulative']=[sum(list(temp[0])[0:x]) for x in np.arange(1,len(list(temp[0]))+1,1)]\n",
    "\n",
    "ax2.plot(temp['deathYR_half'],temp['cumulative']/sum(temp[0]),color=color_yellow,alpha=0.9)\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.set_title('(b) Percentage of Death Occurence in Timeline for 2006 Entry')\n",
    "ax2.set_xlabel('Death Year',size=fontsize_labels)\n",
    "ax2.set_ylabel('Percentage',size=fontsize_labels)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=fontsize_ticks)\n",
    "# row=1,1\n",
    "\n",
    "ax2_1 = plt.subplot2grid((3, 4), (1, 2), colspan=2)\n",
    "temp=df_deaths_time.loc[df_deaths_time['interview_year']==2006,]\n",
    "temp=temp.sort_values(by='deathYR_half')\n",
    "\n",
    "ax2_1.bar(temp['deathYR_half'],temp[0],width=0.3,color=color_blue,alpha=0.8)\n",
    "ax2_1.set_ylim(0,170)\n",
    "ax2_1.set_title('(c) Death Occurence in Timeline for 2006 Entry')\n",
    "ax2_1.set_xlabel('Death Year',size=fontsize_labels)\n",
    "ax2_1.set_ylabel('Death Occurrence',size=fontsize_labels)\n",
    "ax2_1.spines['top'].set_visible(False)\n",
    "ax2_1.spines['right'].set_visible(False)\n",
    "ax2_1.tick_params(axis='both', which='major', labelsize=fontsize_ticks)\n",
    "\n",
    "# The last one is spread on 1 column only, on the 4th column of the second line.\n",
    "ax3 = plt.subplot2grid((3, 4), (2, 0), colspan=2)\n",
    "temp=df_deaths_time.loc[df_deaths_time['interview_year']==2008,]\n",
    "temp=temp.sort_values(by='deathYR_half')\n",
    "temp['cumulative']=[sum(list(temp[0])[0:x]) for x in np.arange(1,len(list(temp[0]))+1,1)]\n",
    "\n",
    "\n",
    "ax3.set_title('(d) Percentage of Death Occurence in Timeline for 2008 Entry')\n",
    "ax3.plot(temp['deathYR_half'],temp['cumulative']/sum(temp[0]),color=color_yellow,alpha=0.9)\n",
    "ax3.set_ylim(0,1)\n",
    "ax3.set_xlabel('Death Year',size=fontsize_labels)\n",
    "ax3.set_ylabel('Percentage',size=fontsize_labels)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax3.tick_params(axis='both', which='major', labelsize=fontsize_ticks)\n",
    "\n",
    "ax3_1 = plt.subplot2grid((3, 4), (2, 2), colspan=2)\n",
    "ax3_1.set_ylim(0,170)\n",
    "ax3_1.set_title('(e) Death Occurence in Timeline for 2008 Entry')\n",
    "ax3_1.bar(temp['deathYR_half'],temp[0],width=0.3,color=color_blue,alpha=0.8)\n",
    "ax3_1.set_xlabel('Death Year',size=fontsize_labels)\n",
    "ax3_1.set_ylabel('Death Occurrence',size=fontsize_labels)\n",
    "ax3_1.spines['top'].set_visible(False)\n",
    "ax3_1.spines['right'].set_visible(False)\n",
    "ax3_1.tick_params(axis='both', which='major', labelsize=fontsize_ticks)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "# Show the graph\n",
    "# plt.savefig(Path.cwd()/'graphs/fig1_death_timeline.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Figures-fig2_top_10_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------\n",
    "# Figure 2\n",
    "# ------------------------\n",
    "\n",
    "var_dict=DataImport.variable_dict()\n",
    "top_10_cols_binary=['currsmokeYN','eversmokeYN','alcoholYN','everdivorced','nevermarried','everunemployed']\n",
    "top_10_cols_continuous=['Zlifesatis','Zrecentfindiff','Znegaffect']\n",
    "\n",
    "for var in top_10_cols_binary:\n",
    "    positive_label_us = [1 if x >0 else 0 for x in df_by_us[var]]\n",
    "    positive_label = [1 if x > 0 else 0 for x in df[var]]\n",
    "\n",
    "    print(var_dict[var],'&',round(df_by_us[var].mean(),3),'&',round(df[var].mean(),3),'&',round(np.mean(positive_label_us),3),'&',round(np.mean(positive_label),3), '\\\\\\\\')\n",
    "\n",
    "\n",
    "fontsize_ticks = 12\n",
    "fontsize_labels = 13\n",
    "fig,axis = plt.subplots(3,3)\n",
    "plt.rcParams['figure.figsize']=[13,12]\n",
    "fig.subplots_adjust(left=0.06, bottom=0.08,top=0.95,right=0.99)\n",
    "count = 0\n",
    "colors=[color_blue,color_yellow]\n",
    "for (m,n),subplot in np.ndenumerate(axis):\n",
    "    var = top_10_cols_continuous[count]\n",
    "    if n ==0:\n",
    "\n",
    "        axis[m,n].hist(df[var],bins=[231,29,238][count],color=color_yellow,alpha=0.9)\n",
    "        axis[m, n].set_xlabel(var_dict[var],size=fontsize_labels)\n",
    "        axis[m, n].set_ylabel('Counts',size=fontsize_labels)\n",
    "        if m ==0:\n",
    "            axis[m,n].set_title('Author Data')\n",
    "\n",
    "    elif n==1:\n",
    "        axis[m, n].hist(df_by_us[var], bins=[231,29,238][count],color=color_blue,alpha=0.8)\n",
    "        axis[m, n].set_xlabel(var_dict[var],size=fontsize_labels)\n",
    "        axis[m, n].set_ylabel('Counts',size=fontsize_labels)\n",
    "        if m ==0:\n",
    "            axis[m,n].set_title('Our Data')\n",
    "    else:\n",
    "        box_1 = axis[m, n].boxplot(df[var], positions=[1], labels=['Author Data'],\n",
    "                           patch_artist=True, boxprops=dict(color=colors[1]), medianprops=dict(color=colors[1]),\n",
    "                           whiskerprops=dict(color=colors[1]), capprops=dict(color=colors[1]), showfliers=False)\n",
    "        box_0 = axis[m, n].boxplot(df_by_us[var], positions=[2], labels=['Our Data'],\n",
    "                           patch_artist=True, boxprops=dict(color=colors[0]), medianprops=dict(color=colors[0]),\n",
    "                           whiskerprops=dict(color=colors[0]), capprops=dict(color=colors[0]), showfliers=False)\n",
    "\n",
    "        box = [box_1, box_0]\n",
    "        # set the face color of boxes, which is the color of box\n",
    "        for i in [0,1]:\n",
    "            for patch in box[i]['boxes']:\n",
    "                patch.set(facecolor=colors[np.abs(i-1)], alpha=0.4)\n",
    "        axis[m, n].set_ylabel('Feature Value')\n",
    "        if m ==0:\n",
    "            axis[m,n].set_title('Box Plot Comparison')\n",
    "        count += 1\n",
    "    axis[m, n].spines['top'].set_visible(False)\n",
    "    axis[m, n].spines['right'].set_visible(False)\n",
    "    axis[m,n].tick_params(axis='both', which='major', labelsize=fontsize_ticks)\n",
    "\n",
    "plt.savefig(Path.cwd()/'graphs/top_10_continuous.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_categorical_vars(column,cat_num,df):\n",
    "    \n",
    "    # pair of values and their number of observations \n",
    "    cat_val_count_lst=[]\n",
    "    count=0\n",
    "    for index,value in df[column].value_counts().sort_values(ascending=False).items():\n",
    "        count +=1\n",
    "        if count>=cat_num+1:\n",
    "            break\n",
    "        else:\n",
    "            # print(index,value)\n",
    "            cat_val_count_lst.append([index,value])\n",
    "    print(cat_val_count_lst)\n",
    "    cat_val_count_lst=sorted(cat_val_count_lst,key=lambda x:x[0])\n",
    "    # cut off calculation \n",
    "    cut_point={}\n",
    "    for index in range(0,cat_num-1):\n",
    "        cat_diff = cat_val_count_lst[index+1][0]-cat_val_count_lst[index][0]\n",
    "        gravity = cat_val_count_lst[index][1]/(cat_val_count_lst[index][1]+cat_val_count_lst[index+1][1])\n",
    "        cut_point[cat_val_count_lst[index][0]] = cat_val_count_lst[index][0]+gravity*cat_diff\n",
    "    # cut bins \n",
    "    cats = list(cut_point.values())\n",
    "    cats.sort()\n",
    "    bins=[cats[0]-100]+cats+[cats[len(cats)-1]+100]\n",
    "    \n",
    "    print('cut_point',cut_point)\n",
    "    #print('cat_val_count_lst',cat_val_count_lst)\n",
    "    #print(cats)\n",
    "    print('bins',bins)\n",
    "    df[column]=pd.cut(df[column],bins=bins,labels=[x[0] for x in cat_val_count_lst])\n",
    "    df[column]=np.asarray(df[column])\n",
    "    \n",
    "    return df\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "237px",
    "width": "540px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
